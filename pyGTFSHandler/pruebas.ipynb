{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da573292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41484/3251617969.py:229: DataOrientationWarning: Row orientation inferred during DataFrame construction. Explicitly specify the orientation by passing `orient=\"row\"` to silence this warning.\n",
      "  df = pl.DataFrame(parsed_rows, schema=header_row)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 250\u001b[39m, in \u001b[36mvalidate_and_load_csv\u001b[39m\u001b[34m(path, header, csv_text)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dtype == \u001b[33m\"\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     parsed_val = \u001b[43mparse_time\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dtype == \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mparse_time\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m h > \u001b[32m47\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHour value over 47: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Hour value over 47: 61",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 308\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    303\u001b[39m     csv_text = \u001b[33m'''\u001b[39m\u001b[33mservice_id,monday,tuesday,wednesday,thursday,friday,saturday,sunday,start_date,end_date\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[33mweekday,True,True,True,True,True,false,False,2025-10-01,20251231\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[33mweekend,False,False,False,False,False,True,True,20251005,20251231\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[33m'''\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     df, errors_df, cfg = \u001b[43mvalidate_and_load_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/miguel/Documents/Proyectos/PTLevelofService/accessibility/UrbanAccessAnalyzer/no_sync/cambridge_massachusetts_us/gtfs_files/mdb11__amtrak/stop_times.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDetected config:\u001b[39m\u001b[33m\"\u001b[39m, cfg)\n\u001b[32m    311\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mErrors:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 275\u001b[39m, in \u001b[36mvalidate_and_load_csv\u001b[39m\u001b[34m(path, header, csv_text)\u001b[39m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# Capture exact parsing error\u001b[39;00m\n\u001b[32m    268\u001b[39m     errors_df = errors_df.vstack(pl.DataFrame({\n\u001b[32m    269\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mline_number\u001b[39m\u001b[33m\"\u001b[39m: [idx + \u001b[32m1\u001b[39m],\n\u001b[32m    270\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[38;5;28mstr\u001b[39m(val)],\n\u001b[32m   (...)\u001b[39m\u001b[32m    273\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mexcluded\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[38;5;28;01mTrue\u001b[39;00m]\n\u001b[32m    274\u001b[39m     }))\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     \u001b[43mrows_to_exclude\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     new_vals.append(val)\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# ------------------------------\n",
    "# DATE PARSER\n",
    "# ------------------------------\n",
    "def parse_date(d):\n",
    "    \"\"\"\n",
    "    Parse a date from various formats and return an int in YYYYMMDD format.\n",
    "    \"\"\"\n",
    "    if isinstance(d, int):\n",
    "        d = str(d)\n",
    "\n",
    "    date_formats = [\n",
    "        \"%Y%m%d\", \"%Y-%m-%d\", \"%Y%m%d\", \"%Y%m-%d\",\n",
    "        \"%d-%m-%Y\", \"%d/%m/%Y\", \"%Y/%m/%d\", \"%m/%d/%Y\",\n",
    "        \"%m-%d-%Y\", \"%d.%m.%Y\",\n",
    "    ]\n",
    "\n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            parsed_date = datetime.strptime(d, fmt)\n",
    "            return str(parsed_date.strftime(\"%Y%m%d\"))\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    raise ValueError(f\"Could not parse date: {d}\")\n",
    "\n",
    "# ------------------------------\n",
    "# TIME PARSER\n",
    "# ------------------------------\n",
    "def parse_time(t: str) -> str:\n",
    "    \"\"\"\n",
    "    Parse a time string in various GTFS-compatible formats and return HH:MM:SS.\n",
    "    Supports hours up to 47.\n",
    "    \"\"\"\n",
    "    if isinstance(t, int):\n",
    "        t = str(t)\n",
    "    t = t.strip().lower().replace(',', '.')\n",
    "\n",
    "    # Handle AM/PM\n",
    "    ampm_match = re.match(r'(\\d{1,2}):?(\\d{1,2})?:?(\\d{1,2})?\\s*(am|pm)', t)\n",
    "    if ampm_match:\n",
    "        h, m, s, meridiem = ampm_match.groups()\n",
    "        h, m, s = int(h), int(m or 0), int(s or 0)\n",
    "        if meridiem == 'pm' and h < 12:\n",
    "            h += 12\n",
    "        if meridiem == 'am' and h == 12:\n",
    "            h = 0\n",
    "        if h > 47:\n",
    "            raise ValueError(f\"Hour value over 47: {h}\")\n",
    "        return f\"{h:02}:{m:02}:{s:02}\"\n",
    "\n",
    "    # Split by colon if present\n",
    "    parts = re.split(r'[:]', t)\n",
    "    if 1 <= len(parts) <= 3:\n",
    "        h = int(parts[0]) if parts[0] else 0\n",
    "        m = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n",
    "        s = int(parts[2]) if len(parts) > 2 and parts[2] else 0\n",
    "        if h > 47:\n",
    "            raise ValueError(f\"Hour value over 47: {h}\")\n",
    "        return f\"{h:02}:{m:02}:{s:02}\"\n",
    "\n",
    "    # Fallback: digits-only string\n",
    "    digits = ''.join(c for c in t if c.isdigit())\n",
    "    if digits:\n",
    "        digits = digits.zfill(6)\n",
    "        h, m, s = int(digits[:2]), int(digits[2:4]), int(digits[4:6])\n",
    "        if h > 47:\n",
    "            raise ValueError(f\"Hour value over 47: {h}\")\n",
    "        return f\"{h:02}:{m:02}:{s:02}\"\n",
    "\n",
    "    raise ValueError(f\"Could not parse time: {t}\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# SCHEMA DEFINITION\n",
    "# ------------------------------\n",
    "def get_df_schema_dict(path: str) -> Tuple[Dict[str, Any], List[str]]:\n",
    "    if \"stops.txt\" in str(path):\n",
    "        schema_dict = {\"stop_id\": str, \"stop_name\": str, \"stop_lat\": float, \"stop_lon\": float}\n",
    "        mandatory_cols = [\"stop_id\", \"stop_lat\", \"stop_lon\"]\n",
    "    elif \"trips.txt\" in str(path):\n",
    "        schema_dict = {\"route_id\": str, \"service_id\": str, \"trip_id\": str}\n",
    "        mandatory_cols = [\"route_id\", \"service_id\", \"trip_id\"]\n",
    "    elif \"stop_times.txt\" in str(path):\n",
    "        schema_dict = {\n",
    "            \"trip_id\": str,\n",
    "            \"arrival_time\": \"time\",\n",
    "            \"departure_time\": \"time\",\n",
    "            \"stop_id\": str,\n",
    "            \"stop_sequence\": int\n",
    "        }\n",
    "        mandatory_cols = [\"trip_id\", \"arrival_time\", \"departure_time\", \"stop_id\"]\n",
    "    elif \"routes.txt\" in str(path):\n",
    "        schema_dict = {\"route_id\": str, \"agency_id\": str, \"route_short_name\": str,\n",
    "                       \"route_long_name\": str, \"route_type\": int}\n",
    "        mandatory_cols = [\"route_id\", \"route_type\"]\n",
    "    elif \"calendar.txt\" in str(path):\n",
    "        schema_dict = {\n",
    "            \"service_id\": str, \"monday\": int|bool, \"tuesday\": int|bool, \"wednesday\": int|bool,\n",
    "            \"thursday\": int|bool, \"friday\": int|bool, \"saturday\": int|bool, \"sunday\": int|bool,\n",
    "            \"start_date\": \"date\", \"end_date\": \"date\"\n",
    "        }\n",
    "        mandatory_cols = [\"service_id\", \"monday\", \"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\",\"start_date\",\"end_date\"]\n",
    "    else:\n",
    "        raise Exception(f\"File {path} not implemented.\")\n",
    "    return schema_dict, mandatory_cols\n",
    "\n",
    "# ------------------------------\n",
    "# CSV FORMAT DETECTION\n",
    "# ------------------------------\n",
    "def detect_csv_format(sample_text: str, max_lines: int = 1) -> Dict[str, Any]:\n",
    "    lines = sample_text.strip().splitlines()[:max_lines]\n",
    "    sample = \"\\n\".join(lines)\n",
    "    try:\n",
    "        sniffer = csv.Sniffer()\n",
    "        dialect = sniffer.sniff(sample, delimiters=[',', ';', '\\t', '|'])\n",
    "        delimiter, quotechar, doublequote = dialect.delimiter, dialect.quotechar, dialect.doublequote\n",
    "    except Exception:\n",
    "        possible_delims = [',', ';', '\\t', '|']\n",
    "        delim_scores = {}\n",
    "        for d in possible_delims:\n",
    "            counts = [ln.count(d) for ln in lines if ln.strip()]\n",
    "            if counts:\n",
    "                variance = max(counts) - min(counts)\n",
    "                delim_scores[d] = (sum(counts)/len(counts), variance)\n",
    "        delimiter = min(delim_scores, key=lambda k: delim_scores[k][1]) if delim_scores else ','\n",
    "        quote_candidates = ['\"', \"'\"]\n",
    "        qcounts = {q: sample.count(q) for q in quote_candidates}\n",
    "        quotechar = max(qcounts, key=qcounts.get) if max(qcounts.values()) > 0 else '\"'\n",
    "        doublequote = (quotechar*2) in sample\n",
    "    dot_nums = len(re.findall(r'\\d+\\.\\d+', sample))\n",
    "    comma_nums = len(re.findall(r'\\d+,\\d+', sample))\n",
    "    float_point = '.' if dot_nums >= comma_nums else ','\n",
    "    return {\"delimiter\": delimiter, \"quotechar\": quotechar, \"doublequote\": doublequote, \"float_point\": float_point}\n",
    "\n",
    "# ------------------------------\n",
    "# TRY PARSE SINGLE LINE\n",
    "# ------------------------------\n",
    "def try_parse_line(line: str, config: Dict[str, Any], expected_cols: int = None) -> Tuple[List[str]|None, str|None, str|None, bool, str|None]:\n",
    "    try:\n",
    "        parsed = next(csv.reader([line], delimiter=config[\"delimiter\"], quotechar=config[\"quotechar\"], doublequote=config[\"doublequote\"]))\n",
    "    except Exception as e:\n",
    "        fixed_line = re.sub(r'(?<=\\w)\"(?=\\w)', \"'\", line)\n",
    "        try:\n",
    "            parsed_fixed = next(csv.reader([fixed_line], delimiter=config[\"delimiter\"], quotechar=config[\"quotechar\"], doublequote=config[\"doublequote\"]))\n",
    "            return parsed_fixed, \"Quotation error\", \"replaced embedded \\\" with '\", False, fixed_line\n",
    "        except Exception:\n",
    "            return None, f\"Quotation error: {e}\", \"excluded\", True, None\n",
    "    detected_cols = len(parsed)\n",
    "    if expected_cols is not None and detected_cols != expected_cols:\n",
    "        fixed_line = re.sub(r'(?<=\\w)\"(?=\\w)', \"'\", line)\n",
    "        try:\n",
    "            parsed_fixed = next(csv.reader([fixed_line], delimiter=config[\"delimiter\"], quotechar=config[\"quotechar\"], doublequote=config[\"doublequote\"]))\n",
    "            if len(parsed_fixed) == expected_cols:\n",
    "                return parsed_fixed, f\"expected {expected_cols} cols, got {detected_cols}\", \"replaced embedded \\\" with '\", False, fixed_line\n",
    "        except Exception:\n",
    "            pass\n",
    "        return None, f\"expected {expected_cols} cols, got {detected_cols}\", \"excluded\", True, None\n",
    "    return parsed, None, None, False, line\n",
    "\n",
    "# ------------------------------\n",
    "# VALIDATE & LOAD CSV\n",
    "# ------------------------------\n",
    "def validate_and_load_csv(path: str, header: bool = True, csv_text=None) -> Tuple[pl.DataFrame|None, pl.DataFrame, Dict[str,Any]|str]:\n",
    "    schema_dict, mandatory_cols = get_df_schema_dict(path)\n",
    "\n",
    "    if csv_text is None:\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(path)\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            csv_text = f.read()\n",
    "\n",
    "    lines = csv_text.splitlines()\n",
    "    config = detect_csv_format(csv_text)\n",
    "\n",
    "    parsed_rows = []\n",
    "    expected_cols = None\n",
    "    header_row = None\n",
    "\n",
    "    # Initialize consistent empty errors_df\n",
    "    errors_df = pl.DataFrame({\n",
    "        \"line_number\": pl.Series(dtype=pl.Int64),\n",
    "        \"content\": pl.Series(dtype=pl.Utf8),\n",
    "        \"error\": pl.Series(dtype=pl.Utf8),\n",
    "        \"fix\": pl.Series(dtype=pl.Utf8),\n",
    "        \"excluded\": pl.Series(dtype=pl.Boolean),\n",
    "    })\n",
    "\n",
    "    for i, line in enumerate(lines, start=1):\n",
    "        parsed, error, fix, excluded, used_line = try_parse_line(line, config, expected_cols)\n",
    "        if parsed is not None and expected_cols is None:\n",
    "            expected_cols = len(parsed)\n",
    "\n",
    "        if header and parsed is not None and header_row is None:\n",
    "            header_row = parsed\n",
    "            expected_cols = len(header_row)\n",
    "            continue\n",
    "\n",
    "        if parsed is not None:\n",
    "            parsed_rows.append(parsed)\n",
    "\n",
    "        if error:\n",
    "            new_error = pl.DataFrame({\n",
    "                \"line_number\": [i],\n",
    "                \"content\": [line],\n",
    "                \"error\": [error],\n",
    "                \"fix\": [fix],\n",
    "                \"excluded\": [excluded],\n",
    "            })\n",
    "            errors_df = errors_df.vstack(new_error)\n",
    "\n",
    "    if not header_row:\n",
    "        return None, pl.DataFrame([{\"error\": \"No header found\"}]), config\n",
    "\n",
    "    present_cols = set(header_row)\n",
    "    missing = [c for c in mandatory_cols if c not in present_cols]\n",
    "    if missing:\n",
    "        err_str = f\"Mandatory cols {missing} not in file\"\n",
    "        return None, pl.DataFrame([{\"error\": err_str}]), err_str\n",
    "\n",
    "    df = pl.DataFrame(parsed_rows, schema=header_row)\n",
    "\n",
    "    rows_to_exclude = set()\n",
    "    for col, dtype in schema_dict.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        new_vals = []\n",
    "\n",
    "        for idx, val in enumerate(df[col].to_list()):\n",
    "            original = val\n",
    "            parsed_val = val\n",
    "\n",
    "            if val is None or str(val).strip() == '':\n",
    "                new_vals.append(val)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                if dtype == \"date\":\n",
    "                    parsed_val = parse_date(str(val))\n",
    "                elif dtype == \"time\":\n",
    "                    parsed_val = parse_time(str(val))\n",
    "                elif dtype == int | bool:\n",
    "                    sval = str(val).strip().lower()\n",
    "                    if sval in (\"true\",\"1\"):\n",
    "                        parsed_val = 1\n",
    "                    elif sval in (\"false\",\"0\"):\n",
    "                        parsed_val = 0\n",
    "                    else:\n",
    "                        parsed_val = int(float(sval))\n",
    "                elif dtype == int:\n",
    "                    parsed_val = int(float(val))\n",
    "                elif dtype == float:\n",
    "                    parsed_val = float(val)\n",
    "                else:\n",
    "                    parsed_val = str(val)\n",
    "\n",
    "            except Exception as e:\n",
    "                # Capture exact parsing error\n",
    "                errors_df = errors_df.vstack(pl.DataFrame({\n",
    "                    \"line_number\": [idx + 1],\n",
    "                    \"content\": [str(val)],\n",
    "                    \"error\": [f\"Parse failed for column '{col}': {e}\"],\n",
    "                    \"fix\": [\"excluded\"],\n",
    "                    \"excluded\": [True]\n",
    "                }))\n",
    "                rows_to_exclude.add(idx)\n",
    "                new_vals.append(val)\n",
    "                continue\n",
    "\n",
    "            # Track modifications\n",
    "            if str(parsed_val) != str(original):\n",
    "                errors_df = errors_df.vstack(pl.DataFrame({\n",
    "                    \"line_number\": [idx + 1],\n",
    "                    \"content\": [str(original)],\n",
    "                    \"error\": [f\"Value in column '{col}' modified after parsing\"],\n",
    "                    \"fix\": [f\"{original} -> {parsed_val}\"],\n",
    "                    \"excluded\": [False]\n",
    "                }))\n",
    "\n",
    "            new_vals.append(parsed_val)\n",
    "\n",
    "        df = df.with_columns(pl.Series(col, new_vals))\n",
    "\n",
    "    if rows_to_exclude:\n",
    "        df = df.filter(~pl.arange(0, df.height).is_in(list(rows_to_exclude)))\n",
    "\n",
    "    return df, errors_df, config\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Example usage\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    csv_text = '''service_id,monday,tuesday,wednesday,thursday,friday,saturday,sunday,start_date,end_date\n",
    "weekday,True,True,True,True,True,false,False,2025-10-01,20251231\n",
    "weekend,False,False,False,False,False,True,True,20251005,20251231\n",
    "'''\n",
    "\n",
    "    df, errors_df, cfg = validate_and_load_csv(\"/home/miguel/Documents/Proyectos/PTLevelofService/accessibility/UrbanAccessAnalyzer/no_sync/cambridge_massachusetts_us/gtfs_files/mdb11__amtrak/stop_times.txt\", header=True)\n",
    "\n",
    "    print(\"Detected config:\", cfg)\n",
    "    print(\"\\nErrors:\")\n",
    "    print(errors_df)\n",
    "    print(\"\\nLoaded data:\")\n",
    "    print(df)\n",
    "    print(\"\\nLoaded shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb52f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ac3b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file, ext = os.path.splitext(\"a/b/c.txt\")    # file = \"c\", ext = \".txt\"\n",
    "ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce59d07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c980b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     csv_text = '''stop_id,stop_name,x,stop_lat,stop_lon,y,z,a,b,c \n",
    "#     839,\"Rt. 340 at Pine Hill Farms\",,40.0386666852,-76.1498668113,,,0,,0 \n",
    "# 839,\"Rt. 340 at Pine Hill Farms\",,40.0386666852,-76.1498668113,,,0,,0\n",
    "# 1441;\"Rt. 30 at Rhoades Auto\";;40.0141079301;-76.1507757729;;;0;;0 \n",
    "# 1,\"Rt. 30 at King\"s\",,40.0144107263,-76.1523472101,,,0,,0\n",
    "# 1441,\"Rt. 30 at Rhoades Auto\",,40.0141079301,-76.1507757729,,,0,,0\n",
    "# 1442,\"Rt. 30 at After Eight Bed & Breakfast\",,40.0138175168,-76.149297822,,,0,,0\n",
    "# 1443,\"Rt. 30 at Paradise Custom Shop\",,40.0131079771,-76.1456821665,,,0,,0\n",
    "# 1444,\"Rt. 30 at Paradise Candle\",,40.0124615832,-76.1424119972,,,0,,0\n",
    "# 1446,\"Rt. 30 at hex Shop\",,40.0116494174,-76.1383344392,,,0,,0\n",
    "# 839,\"Rt. 340 at Pine Hill Farms\",,40.0386666852,-76.1498668113,,,0,,0\n",
    "# 84,\"Fellowship Dr. & Friendship Ave.'''  # truncated intentionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "405d788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_time(t: str) -> str:\n",
    "    \"\"\"\n",
    "    Parse a time string and return HH:MM:SS.\n",
    "    Rules:\n",
    "      - With colons (:): parse flexibly (e.g., \"7:1\" -> \"07:01:00\").\n",
    "      - Without colons: only 4 or 6 digits are allowed:\n",
    "            4 digits (HHMM) -> HH:MM:00\n",
    "            6 digits (HHMMSS) -> HH:MM:SS\n",
    "      Supports hours up to 47.\n",
    "    \"\"\"\n",
    "    if isinstance(t, int):\n",
    "        t = str(t)\n",
    "    t = t.strip().lower().replace(',', '.')\n",
    "\n",
    "    # Handle AM/PM (unchanged)\n",
    "    ampm_match = re.match(r'(\\d{1,2}):?(\\d{1,2})?:?(\\d{1,2})?\\s*(am|pm)', t)\n",
    "    if ampm_match:\n",
    "        h, m, s, meridiem = ampm_match.groups()\n",
    "        h, m, s = int(h), int(m or 0), int(s or 0)\n",
    "        if meridiem == 'pm' and h < 12:\n",
    "            h += 12\n",
    "        if meridiem == 'am' and h == 12:\n",
    "            h = 0\n",
    "        if h > 47:\n",
    "            raise ValueError(f\"Hour value over 47: {h}\")\n",
    "        return f\"{h:02}:{m:02}:{s:02}\"\n",
    "\n",
    "    # Case 1: String has colons → flexible parsing\n",
    "    if ':' in t:\n",
    "        parts = t.split(':')\n",
    "        parts = [p.zfill(2) if p else '00' for p in parts]\n",
    "        while len(parts) < 3:\n",
    "            parts.append('00')\n",
    "\n",
    "        h, m, s = map(int, parts[:3])\n",
    "        if m > 59 or s > 59:\n",
    "            raise ValueError(f\"Invalid time value: {t}\")\n",
    "        if h > 47:\n",
    "            raise ValueError(f\"Invalid hour value: {t} is over 47 hours\")\n",
    "\n",
    "        return f\"{h:02}:{m:02}:{s:02}\"\n",
    "\n",
    "    # Case 2: No colons → must be 4 or 6 digits\n",
    "    digits = ''.join(c for c in t if c.isdigit())\n",
    "    if not digits:\n",
    "        raise ValueError(f\"Could not parse time: {t}\")\n",
    "\n",
    "    if len(digits) == 4:  # HHMM\n",
    "        h, m, s = int(digits[:2]), int(digits[2:4]), 0\n",
    "    elif len(digits) == 6:  # HHMMSS\n",
    "        h, m, s = int(digits[:2]), int(digits[2:4]), int(digits[4:6])\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid time format (must be 4 or 6 digits): {t}\")\n",
    "\n",
    "    if m > 59 or s > 59:\n",
    "        raise ValueError(f\"Invalid time value: {t}\")\n",
    "    if h > 47:\n",
    "        raise ValueError(f\"Invalid hour value: {t} is over 47 hours\")\n",
    "\n",
    "    return f\"{h:02}:{m:02}:{s:02}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "557fc4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01:00:00'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_time(\"30:02:01\")    # → '30:02:01'\n",
    "parse_time(\"7:01:02\")     # → '07:01:02'\n",
    "parse_time(\"07:01\")       # → '07:01:00'\n",
    "parse_time(\"0710\")       # → '07:01:02'\n",
    "parse_time(\"11:59:59 PM\") # → '23:59:59'\n",
    "parse_time(\"1:00:00\")   # → '100:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6aea9ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid hour value: 7010 is over 47 hours",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mparse_time\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m7010\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# → '30:02:01'\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mparse_time\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid time value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m h > \u001b[32m47\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid hour value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is over 47 hours\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Invalid hour value: 7010 is over 47 hours"
     ]
    }
   ],
   "source": [
    "parse_time(\"7010\")    # → '30:02:01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4ca628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import warnings \n",
    "\n",
    "# ------------------------------\n",
    "# DATE PARSER\n",
    "# ------------------------------\n",
    "def parse_date(d):\n",
    "    \"\"\"\n",
    "    Parse a date from various formats and return an int in YYYYMMDD format.\n",
    "    \"\"\"\n",
    "    if isinstance(d, int):\n",
    "        d = str(d)\n",
    "\n",
    "    date_formats = [\n",
    "        \"%Y%m%d\", \"%Y-%m-%d\", \"%Y%m%d\", \"%Y%m-%d\",\n",
    "        \"%d-%m-%Y\", \"%d/%m/%Y\", \"%Y/%m/%d\", \"%m/%d/%Y\",\n",
    "        \"%m-%d-%Y\", \"%d.%m.%Y\",\n",
    "    ]\n",
    "\n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            parsed_date = datetime.strptime(d, fmt)\n",
    "            return str(parsed_date.strftime(\"%Y%m%d\"))\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    raise ValueError(f\"Could not parse date: {d}\")\n",
    "\n",
    "# ------------------------------\n",
    "# TIME PARSER\n",
    "# ------------------------------\n",
    "def parse_time(t: str) -> str:\n",
    "    \"\"\"\n",
    "    Parse a time string and return HH:MM:SS.\n",
    "    Rules:\n",
    "      - With colons (:): parse flexibly (e.g., \"7:1\" -> \"07:01:00\").\n",
    "      - Without colons: only 4 or 6 digits are allowed:\n",
    "            4 digits (HHMM) -> HH:MM:00\n",
    "            6 digits (HHMMSS) -> HH:MM:SS\n",
    "      Supports hours up to 47.\n",
    "    \"\"\"\n",
    "    if isinstance(t, int):\n",
    "        t = str(t)\n",
    "    t = t.strip().lower().replace(',', '.')\n",
    "\n",
    "    # Handle AM/PM (unchanged)\n",
    "    ampm_match = re.match(r'(\\d{1,2}):?(\\d{1,2})?:?(\\d{1,2})?\\s*(am|pm)', t)\n",
    "    if ampm_match:\n",
    "        h, m, s, meridiem = ampm_match.groups()\n",
    "        h, m, s = int(h), int(m or 0), int(s or 0)\n",
    "        if meridiem == 'pm' and h < 12:\n",
    "            h += 12\n",
    "        if meridiem == 'am' and h == 12:\n",
    "            h = 0\n",
    "        if h > 47:\n",
    "            raise ValueError(f\"Hour value over 47: {h}\")\n",
    "        return f\"{h:02}:{m:02}:{s:02}\"\n",
    "\n",
    "    # Case 1: String has colons → flexible parsing\n",
    "    if ':' in t:\n",
    "        parts = t.split(':')\n",
    "        parts = [p.zfill(2) if p else '00' for p in parts]\n",
    "        while len(parts) < 3:\n",
    "            parts.append('00')\n",
    "\n",
    "        h, m, s = map(int, parts[:3])\n",
    "        if m > 59 or s > 59:\n",
    "            raise ValueError(f\"Invalid time value: {t}\")\n",
    "        if h > 47:\n",
    "            raise ValueError(f\"Invalid hour value: {t} is over 47 hours\")\n",
    "\n",
    "        return f\"{h:02}:{m:02}:{s:02}\"\n",
    "\n",
    "    # Case 2: No colons → must be 4 or 6 digits\n",
    "    digits = ''.join(c for c in t if c.isdigit())\n",
    "    if not digits:\n",
    "        raise ValueError(f\"Could not parse time: {t}\")\n",
    "\n",
    "    if len(digits) == 4:  # HHMM\n",
    "        h, m, s = int(digits[:2]), int(digits[2:4]), 0\n",
    "    elif len(digits) == 6:  # HHMMSS\n",
    "        h, m, s = int(digits[:2]), int(digits[2:4]), int(digits[4:6])\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid time format (must be 4 or 6 digits): {t}\")\n",
    "\n",
    "    if m > 59 or s > 59:\n",
    "        raise ValueError(f\"Invalid time value: {t}\")\n",
    "    if h > 47:\n",
    "        raise ValueError(f\"Invalid hour value: {t} is over 47 hours\")\n",
    "\n",
    "    return f\"{h:02}:{m:02}:{s:02}\"\n",
    "\n",
    "# ------------------------------\n",
    "# SCHEMA DEFINITION\n",
    "# ------------------------------\n",
    "def get_df_schema_dict(path: str) -> Tuple[Dict[str, Any], List[str]]:\n",
    "    if \"stops.txt\" in str(path):\n",
    "        schema_dict = {\"stop_id\": str, \"stop_name\": str, \"stop_lat\": float, \"stop_lon\": float}\n",
    "        mandatory_cols = [\"stop_id\", \"stop_lat\", \"stop_lon\"]\n",
    "    elif \"trips.txt\" in str(path):\n",
    "        schema_dict = {\"route_id\": str, \"service_id\": str, \"trip_id\": str}\n",
    "        mandatory_cols = [\"route_id\", \"service_id\", \"trip_id\"]\n",
    "    elif \"stop_times.txt\" in str(path):\n",
    "        schema_dict = {\n",
    "            \"trip_id\": str,\n",
    "            \"arrival_time\": \"time\",\n",
    "            \"departure_time\": \"time\",\n",
    "            \"stop_id\": str,\n",
    "            \"stop_sequence\": int\n",
    "        }\n",
    "        mandatory_cols = [\"trip_id\", \"arrival_time\", \"departure_time\", \"stop_id\"]\n",
    "    elif \"routes.txt\" in str(path):\n",
    "        schema_dict = {\"route_id\": str, \"agency_id\": str, \"route_short_name\": str,\n",
    "                       \"route_long_name\": str, \"route_type\": int}\n",
    "        mandatory_cols = [\"route_id\", \"route_type\"]\n",
    "    elif \"calendar.txt\" in str(path):\n",
    "        schema_dict = {\n",
    "            \"service_id\": str, \"monday\": int|bool, \"tuesday\": int|bool, \"wednesday\": int|bool,\n",
    "            \"thursday\": int|bool, \"friday\": int|bool, \"saturday\": int|bool, \"sunday\": int|bool,\n",
    "            \"start_date\": \"date\", \"end_date\": \"date\"\n",
    "        }\n",
    "        mandatory_cols = [\"service_id\", \"monday\", \"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\",\"start_date\",\"end_date\"]\n",
    "    else:\n",
    "        raise Exception(f\"File {path} not implemented.\")\n",
    "    return schema_dict, mandatory_cols\n",
    "\n",
    "# ------------------------------\n",
    "# CSV FORMAT DETECTION\n",
    "# ------------------------------\n",
    "def detect_csv_format(sample_text: str, max_lines: int = 1) -> Dict[str, Any]:\n",
    "    lines = sample_text.strip().splitlines()[:max_lines]\n",
    "    sample = \"\\n\".join(lines)\n",
    "    try:\n",
    "        sniffer = csv.Sniffer()\n",
    "        dialect = sniffer.sniff(sample, delimiters=[',', ';', '\\t', '|'])\n",
    "        delimiter, quotechar, doublequote = dialect.delimiter, dialect.quotechar, dialect.doublequote\n",
    "    except Exception:\n",
    "        possible_delims = [',', ';', '\\t', '|']\n",
    "        delim_scores = {}\n",
    "        for d in possible_delims:\n",
    "            counts = [ln.count(d) for ln in lines if ln.strip()]\n",
    "            if counts:\n",
    "                variance = max(counts) - min(counts)\n",
    "                delim_scores[d] = (sum(counts)/len(counts), variance)\n",
    "        delimiter = min(delim_scores, key=lambda k: delim_scores[k][1]) if delim_scores else ','\n",
    "        quote_candidates = ['\"', \"'\"]\n",
    "        qcounts = {q: sample.count(q) for q in quote_candidates}\n",
    "        quotechar = max(qcounts, key=qcounts.get) if max(qcounts.values()) > 0 else '\"'\n",
    "        doublequote = (quotechar*2) in sample\n",
    "    dot_nums = len(re.findall(r'\\d+\\.\\d+', sample))\n",
    "    comma_nums = len(re.findall(r'\\d+,\\d+', sample))\n",
    "    float_point = '.' if dot_nums >= comma_nums else ','\n",
    "    return {\"delimiter\": delimiter, \"quotechar\": quotechar, \"doublequote\": doublequote, \"float_point\": float_point}\n",
    "\n",
    "# ------------------------------\n",
    "# TRY PARSE SINGLE LINE\n",
    "# ------------------------------\n",
    "def try_parse_line(line: str, config: Dict[str, Any], expected_cols: int|None = None, header:list|None=None, schema:dict|None=None) -> Tuple[List[str]|None, str|None, str|None, bool]:\n",
    "    parsed = None\n",
    "    error = \"\"\n",
    "    fix = \"\"\n",
    "    try:\n",
    "        parsed = next(csv.reader([line], delimiter=config[\"delimiter\"], quotechar=config[\"quotechar\"], doublequote=config[\"doublequote\"]))\n",
    "    except Exception as e:\n",
    "        fixed_line = re.sub(r'(?<=\\w)\"(?=\\w)', \"'\", line)\n",
    "        try:\n",
    "            parsed = next(csv.reader([fixed_line], delimiter=config[\"delimiter\"], quotechar=config[\"quotechar\"], doublequote=config[\"doublequote\"]))\n",
    "            error += \"Quotation error \"\n",
    "            fix += \"replaced embedded \\\" with ' \"\n",
    "        except Exception:\n",
    "            return None, f\"Quotation error: {e}\", \"excluded\", True\n",
    "        \n",
    "    detected_cols = len(parsed)\n",
    "    if expected_cols is not None and detected_cols != expected_cols:\n",
    "        fixed_line = re.sub(r'(?<=\\w)\"(?=\\w)', \"'\", line)\n",
    "        try:\n",
    "            parsed = next(csv.reader([fixed_line], delimiter=config[\"delimiter\"], quotechar=config[\"quotechar\"], doublequote=config[\"doublequote\"]))\n",
    "            if len(parsed) == expected_cols:\n",
    "                error += f\"expected {expected_cols} cols, got {detected_cols} \"\n",
    "                fix += \"replaced embedded \\\" with ' \"\n",
    "        except Exception:\n",
    "            error += f\"expected {expected_cols} cols, got {detected_cols} \"\n",
    "            fix += \"excluded \"\n",
    "            return None, error, fix, True\n",
    "    \n",
    "    if schema is not None and header is not None and parsed is not None:\n",
    "        for col_idx, col_name in enumerate(header):\n",
    "            if col_name not in schema:\n",
    "                continue\n",
    "\n",
    "            dtype = schema[col_name]\n",
    "\n",
    "            val = parsed[col_idx]\n",
    "            original = val\n",
    "            parsed_val = val\n",
    "\n",
    "            # Skip empty values\n",
    "            if val is None or str(val).strip() == '':\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                if dtype == \"date\":\n",
    "                    parsed_val = parse_date(str(val))\n",
    "                elif dtype == \"time\":\n",
    "                    parsed_val = parse_time(str(val))\n",
    "                elif dtype == int | bool:\n",
    "                    sval = str(val).strip().lower()\n",
    "                    if sval in (\"true\", \"1\"):\n",
    "                        parsed_val = 1\n",
    "                    elif sval in (\"false\", \"0\"):\n",
    "                        parsed_val = 0\n",
    "                    else:\n",
    "                        parsed_val = int(float(sval))\n",
    "                elif dtype == int:\n",
    "                    parsed_val = int(float(val))\n",
    "                elif dtype == float:\n",
    "                    parsed_val = float(val)\n",
    "                else:\n",
    "                    parsed_val = str(val)\n",
    "            except Exception as e:\n",
    "                error += f\"Parse failed for column '{col_name}': {e} \"\n",
    "                fix += \"excluded \"\n",
    "                return None, error, fix, True\n",
    "\n",
    "            # Track modifications\n",
    "            if str(parsed_val) != str(original):\n",
    "                error += f\"Value in column '{col_name}' modified after parsing \"\n",
    "                fix += f\"{original} -> {parsed_val} \"\n",
    "\n",
    "            # Apply modification directly to parsed list\n",
    "            parsed[col_idx] = str(parsed_val)\n",
    "\n",
    "\n",
    "    return parsed, None, None, False\n",
    "\n",
    "\n",
    "def validate_and_load_csv(path: str, header: bool = True, csv_text=None):\n",
    "    # Get schema info\n",
    "    schema_dict, mandatory_cols = get_df_schema_dict(path)\n",
    "\n",
    "    # Read CSV text if not provided\n",
    "    if csv_text is None:\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            csv_text = f.read()\n",
    "\n",
    "    lines = csv_text.splitlines()\n",
    "    config = detect_csv_format(csv_text)\n",
    "\n",
    "    expected_cols = None\n",
    "    df_cols = None\n",
    "\n",
    "    if header:\n",
    "        header_line = lines[0]\n",
    "        lines = lines[1:]\n",
    "        df_cols, error_msg, fix, error = try_parse_line(header_line, config)\n",
    "        if error or (df_cols is None):\n",
    "            raise Exception(f\"Error parsing header of file {path}: {error_msg} {fix}\")\n",
    "        elif error_msg is not None:\n",
    "            warnings.warn(f\"Warning parsing header of file {path}: {error_msg} {fix}\")\n",
    "\n",
    "        if mandatory_cols is not None:\n",
    "            for i in mandatory_cols:\n",
    "                if i not in df_cols:\n",
    "                    raise Exception(f\"Column {i} not in file {path}\")\n",
    "            \n",
    "        expected_cols = len(df_cols)\n",
    "\n",
    "    # Build initial Polars DataFrame with line content\n",
    "    lines_df = pl.DataFrame({\n",
    "        \"line_number\": range(1, len(lines) + 1),\n",
    "        \"content\": lines\n",
    "    })\n",
    "\n",
    "    # Parse each line into structured columns\n",
    "    lines_df = lines_df.with_columns(\n",
    "        pl.col(\"content\").map_elements(\n",
    "            lambda line: {\n",
    "                \"parsed\": try_parse_line(line, config, expected_cols, df_cols, schema_dict)[0],\n",
    "                \"error\": try_parse_line(line, config, expected_cols, df_cols, schema_dict)[1],\n",
    "                \"fix\": try_parse_line(line, config, expected_cols, df_cols, schema_dict)[2],\n",
    "                \"excluded\": try_parse_line(line, config, expected_cols, df_cols, schema_dict)[3],\n",
    "            },\n",
    "            return_dtype=pl.Struct({\n",
    "                \"parsed\": pl.List(pl.Utf8),\n",
    "                \"error\": pl.Utf8,\n",
    "                \"fix\": pl.Utf8,\n",
    "                \"excluded\": pl.Boolean,\n",
    "            })\n",
    "        ).alias(\"parsed_struct\")\n",
    "    ).unnest(\"parsed_struct\")\n",
    "\n",
    "    # Build final DataFrame with parsed columns and df_cols as column names\n",
    "    parsed_cols_df = lines_df.filter(~pl.col(\"excluded\")).select(['line_number','parsed'])\n",
    "    if df_cols is not None:\n",
    "        parsed_cols_df = lines_df.select(['line_number','parsed'])\n",
    "        for i, col_name in enumerate(df_cols):\n",
    "            parsed_cols_df = parsed_cols_df.with_columns(\n",
    "                pl.Series(col_name, lines_df[\"parsed\"].list.get(i).cast(pl.Utf8))\n",
    "            )\n",
    "        \n",
    "        parsed_cols_df = parsed_cols_df.drop('parsed')\n",
    "\n",
    "    errors_df = lines_df.select(['line_number','error','fix','excluded']).drop_nulls(\"error\")\n",
    "    if len(errors_df.filter(\"excluded\")) > 0:\n",
    "        warnings.warn(f\"{len(errors_df.filter(\"excluded\"))} rows of file {path} have failed while parsing.\")\n",
    "\n",
    "    return parsed_cols_df, errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a3b39a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41484/3256405939.py:312: UserWarning: 588 rows of file /home/miguel/Documents/Proyectos/PTLevelofService/accessibility/UrbanAccessAnalyzer/no_sync/cambridge_massachusetts_us/gtfs_files/mdb11__amtrak/stop_times.txt have failed while parsing.\n",
      "  warnings.warn(f\"{len(errors_df.filter(\"excluded\"))} rows of file {path} have failed while parsing.\")\n"
     ]
    }
   ],
   "source": [
    "df, error_df = validate_and_load_csv(\"/home/miguel/Documents/Proyectos/PTLevelofService/accessibility/UrbanAccessAnalyzer/no_sync/cambridge_massachusetts_us/gtfs_files/mdb11__amtrak/stop_times.txt\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b41b083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41484/2938741207.py:1: DeprecationWarning: `is_in` with a collection of the same datatype is ambiguous and deprecated.\n",
      "Please use `implode` to return to previous behavior.\n",
      "\n",
      "See https://github.com/pola-rs/polars/issues/22149 for more information.\n",
      "  df.filter(pl.col(\"line_number\").is_in(df['line_number']))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (38_611, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>line_number</th><th>trip_id</th><th>arrival_time</th><th>departure_time</th><th>stop_id</th><th>stop_sequence</th><th>pickup_type</th><th>drop_off_type</th><th>timepoint</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>1</td><td>&quot;181909&quot;</td><td>&quot;01:00:00&quot;</td><td>&quot;01:00:00&quot;</td><td>&quot;LAX&quot;</td><td>&quot;1&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;1&quot;</td></tr><tr><td>2</td><td>&quot;181909&quot;</td><td>&quot;01:41:00&quot;</td><td>&quot;01:41:00&quot;</td><td>&quot;POS&quot;</td><td>&quot;2&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;1&quot;</td></tr><tr><td>3</td><td>&quot;181909&quot;</td><td>&quot;01:54:00&quot;</td><td>&quot;01:54:00&quot;</td><td>&quot;ONA&quot;</td><td>&quot;3&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;1&quot;</td></tr><tr><td>4</td><td>&quot;181909&quot;</td><td>&quot;03:36:00&quot;</td><td>&quot;03:36:00&quot;</td><td>&quot;PSN&quot;</td><td>&quot;4&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;1&quot;</td></tr><tr><td>5</td><td>&quot;181909&quot;</td><td>&quot;05:47:00&quot;</td><td>&quot;05:47:00&quot;</td><td>&quot;YUM&quot;</td><td>&quot;5&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;1&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>38607</td><td>&quot;215634&quot;</td><td>&quot;20:12:00&quot;</td><td>&quot;20:12:00&quot;</td><td>&quot;JEF&quot;</td><td>&quot;6&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;1&quot;</td></tr><tr><td>38608</td><td>&quot;215634&quot;</td><td>&quot;20:57:00&quot;</td><td>&quot;20:57:00&quot;</td><td>&quot;HEM&quot;</td><td>&quot;7&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;1&quot;</td></tr><tr><td>38609</td><td>&quot;215634&quot;</td><td>&quot;21:24:00&quot;</td><td>&quot;21:24:00&quot;</td><td>&quot;WAH&quot;</td><td>&quot;8&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;1&quot;</td></tr><tr><td>38610</td><td>&quot;215634&quot;</td><td>&quot;22:13:00&quot;</td><td>&quot;22:13:00&quot;</td><td>&quot;KWD&quot;</td><td>&quot;9&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;1&quot;</td></tr><tr><td>38611</td><td>&quot;215634&quot;</td><td>&quot;22:45:00&quot;</td><td>&quot;22:45:00&quot;</td><td>&quot;STL&quot;</td><td>&quot;10&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;1&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (38_611, 9)\n",
       "┌────────────┬─────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ line_numbe ┆ trip_id ┆ arrival_t ┆ departure ┆ … ┆ stop_sequ ┆ pickup_ty ┆ drop_off_ ┆ timepoint │\n",
       "│ r          ┆ ---     ┆ ime       ┆ _time     ┆   ┆ ence      ┆ pe        ┆ type      ┆ ---       │\n",
       "│ ---        ┆ str     ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ str       │\n",
       "│ i64        ┆         ┆ str       ┆ str       ┆   ┆ str       ┆ str       ┆ str       ┆           │\n",
       "╞════════════╪═════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 1          ┆ 181909  ┆ 01:00:00  ┆ 01:00:00  ┆ … ┆ 1         ┆ 0         ┆ 0         ┆ 1         │\n",
       "│ 2          ┆ 181909  ┆ 01:41:00  ┆ 01:41:00  ┆ … ┆ 2         ┆ 0         ┆ 0         ┆ 1         │\n",
       "│ 3          ┆ 181909  ┆ 01:54:00  ┆ 01:54:00  ┆ … ┆ 3         ┆ 0         ┆ 0         ┆ 1         │\n",
       "│ 4          ┆ 181909  ┆ 03:36:00  ┆ 03:36:00  ┆ … ┆ 4         ┆ 0         ┆ 0         ┆ 1         │\n",
       "│ 5          ┆ 181909  ┆ 05:47:00  ┆ 05:47:00  ┆ … ┆ 5         ┆ 0         ┆ 0         ┆ 1         │\n",
       "│ …          ┆ …       ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …         │\n",
       "│ 38607      ┆ 215634  ┆ 20:12:00  ┆ 20:12:00  ┆ … ┆ 6         ┆ 0         ┆ 0         ┆ 1         │\n",
       "│ 38608      ┆ 215634  ┆ 20:57:00  ┆ 20:57:00  ┆ … ┆ 7         ┆ 0         ┆ 0         ┆ 1         │\n",
       "│ 38609      ┆ 215634  ┆ 21:24:00  ┆ 21:24:00  ┆ … ┆ 8         ┆ 0         ┆ 0         ┆ 1         │\n",
       "│ 38610      ┆ 215634  ┆ 22:13:00  ┆ 22:13:00  ┆ … ┆ 9         ┆ 0         ┆ 0         ┆ 1         │\n",
       "│ 38611      ┆ 215634  ┆ 22:45:00  ┆ 22:45:00  ┆ … ┆ 10        ┆ 0         ┆ 0         ┆ 1         │\n",
       "└────────────┴─────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(pl.col(\"line_number\").is_in(df['line_number']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8c2b4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5589ac8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygtfshandler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
